{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_all_kijiji_ads_urls.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJ6hZgOJudU2gZmAltDVr3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonprudhomme/rent_guesser/blob/main/get_all_kijiji_ads_urls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6qKrcrfNmVy",
        "outputId": "a277472f-4b7d-49bc-a42e-98a8c1aa2209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "! pip install pip install beautifulsoup4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.6/dist-packages (1.3.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkOSyWBJNW1G"
      },
      "source": [
        "# Kijiji Crawler\n",
        "\n",
        "\"\"\"\n",
        "  Get all the ads urls to be crawled \n",
        "  By Simon Prudhomme\n",
        "  Date 13 oct 2020\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datetime import date"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKdcAqiXh_7J"
      },
      "source": [
        "def get_urls_from_one_page(ads_urls_list, page_url):\n",
        "  #load page\n",
        "  page = requests.get(page_url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  containers = soup.find_all(\"div\", class_=\"info-container\")\n",
        "  #get url\n",
        "  for container in containers:\n",
        "    ads_urls_list.append(container.find('a')['href'])\n",
        "  return ads_urls_list\n",
        "\n",
        "def get_total_number_of_pages_to_crawl():\n",
        "  #load page\n",
        "  page_url = 'https://www.kijiji.ca/b-appartement-condo/quebec/page-{}/c37l9001?ad=offering'.format(1000)\n",
        "  page = requests.get(page_url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  #get max page to crawl\n",
        "  max_page_number = soup.find_all('span', attrs={'class':'selected'})[0].text\n",
        "  print('Number of pages to craw:',max_page_number)\n",
        "  return max_page_number\n",
        "\n",
        "def save_data():  #TODO: add path\n",
        "  data = pd.DataFrame(set(ads_urls_list), columns=['urls'])\n",
        "  filename = 'urls_to_crawls_{}.csv'.format(date.today().strftime(\"%d%m%Y\"))\n",
        "  data.to_csv(filename)\n",
        "\n",
        "def run_kijiji_urls():\n",
        "  #create empty list\n",
        "  ads_urls_list = []\n",
        "  #get max page to crawl\n",
        "  max_page_number = get_total_number_of_pages_to_crawl()\n",
        "  #loop on all the pages\n",
        "  for num in tqdm(range(1,5,1)):#int(max_page_number)+1,1)):\n",
        "    time.sleep(2)\n",
        "    page_url = 'https://www.kijiji.ca/b-appartement-condo/quebec/page-{}/c37l9001?ad=offering'.format(num)\n",
        "    try:\n",
        "      ads_urls_list = get_urls_from_one_page(ads_urls_list, page_url)\n",
        "      print('Urls list length:',len(ads_urls_list))\n",
        "      save_data() #TODO: add path\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "  #TODO delete ads_urls_list\n",
        "  return ads_urls_list"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmHrdHz-SWFo",
        "outputId": "0887ad54-d771-41aa-f069-3e2569e9f5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Test\n",
        "ads_urls_list = run_kijiji_urls()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of pages to craw: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 1/4 [00:07<00:23,  7.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Urls list length: 45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 2/4 [00:16<00:16,  8.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Urls list length: 90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 3/4 [00:23<00:07,  7.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Urls list length: 135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:31<00:00,  7.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Urls list length: 180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}